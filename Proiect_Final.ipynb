{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proiect_Final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abM43h8DylHS",
        "colab_type": "text"
      },
      "source": [
        "> # **Overview**\n",
        "\n",
        "The purpose of this project (created using Google Colab) is to classify encrypted tweets by dialect into either Moldovian or Romanian language. This is a supervised learning problem, as we will be feeding a labelled dataset into the model, that it can learn from, to make future predictions. We will try different aproaches in order to observ which one is better for our dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgP9GiJK0oqC",
        "colab_type": "text"
      },
      "source": [
        "> # **Step 1.1: Understanding our Dataset**\n",
        "\n",
        "For our AI, we will be using:\n",
        "1.   train_samples.txt - the training data samples (contains the IDs and the encrypted tweets);\n",
        "2.   train_labels.txt - the training labels (contains the IDs and the values 0, assigned for Moldavian, or 1, for Romanian);\n",
        "3. validation_samples.txt - the validation data samples (contains the IDs and the encrypted tweets);\n",
        "4. validation_labels.txt - the validation labels (contains the IDs and the values 0, assigned for Moldavian, or 1, for Romanian);\n",
        "5. test_samples.txt - the test data samples.\n",
        "\n",
        "As you can see, every single .txt file contains 2 not named columns: ID and tweet/label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ArhWkAy2OmK",
        "colab_type": "text"
      },
      "source": [
        "> # **Step 1.2: Reading/Importing our Dataset**\n",
        "\n",
        "We will write a function that will help us import our dataset into the program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2_5iLh33fus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Funcție pt a citi fișierele .txt\n",
        "def citeste_fila(fila):\n",
        "    with open(fila, mode=\"r\", encoding=\"latin-1\") as f:\n",
        "        aux = f.readlines()\n",
        "    return aux"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcV9yvYu3tTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Citire Propriu-zisă\n",
        "train_samples = citeste_fila('train_samples.txt')\n",
        "train_labels = citeste_fila('train_labels.txt')\n",
        "validation_samples = citeste_fila('validation_samples.txt')\n",
        "validation_labels = citeste_fila('validation_labels.txt')\n",
        "test_samples = citeste_fila('test_samples.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uvhEVqn4QrD",
        "colab_type": "text"
      },
      "source": [
        "> # **Step 2.1: Bag of Words**\n",
        "\n",
        "The \"*Bag of Words*\" concept is a term used to specify the problems that have a 'bag of words' or a collection of text data that needs to be worked with.  The basic idea of BoW is to take a piece of text and count the frequency of the words in that text. It is important to note that the BoW concept treats each word individually and the order in which the words occur does not matter.\n",
        "\n",
        "Using this process, we can convert a collection of documents to a matrix, with each document being a row and each word (token) being the column, and the corresponding (row, column) values being the frequency of occurrence of each word or token in that document.\n",
        "\n",
        "To handle this, we will be using sklearns \"*countVectorizer*\" method which does the following:\n",
        "*   It tokenizes the string(separates the string into individual words) and gives an integer ID to each token.\n",
        "*   It counts the occurrence of each of those tokens.\n",
        "\n",
        "**IMPORTANT**:\n",
        "*   The \"*countVectorizer*\" method automatically converts all tokenized words to their lower case. Our tweets are encrypted, so, for the purpose of this project, we need to set the \"*lowercase*\" parameter to \"*False*\", as capitalize letters and normal ones represent different things.\n",
        "*   It also ignores all punctuation so that words followed by a punctuation mark (for example: \"hello!\") are not treated differently than the same words not prefixed or suffixed by a punctuation mark (for example: \"hello\"). For our project, this is an useful feature, as our words from the encrypted tweets contain alphanumeric characters.\n",
        "*   The third parameter to take note of is the stop_words parameter. Stop words refer to the most commonly used words in a language. For our dataset, this feature is not appropriate so we will set it to \"*None*\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpWG4NgDDy5Q",
        "colab_type": "text"
      },
      "source": [
        "> # **Step 2.2:  Implementing Bag of Words in scikit-learn**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lhy06d_EWsg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CountVectorizer pt BoW\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Tweets codate -> nu e nevoie de lowercase sau stop_words\n",
        "CV = CountVectorizer(max_features = 50000, lowercase = False, stop_words = None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbHhhs2sFpcT",
        "colab_type": "text"
      },
      "source": [
        "As we recall, every single .txt file contains 2 not named columns: ID and tweet/label; so we need to separate them in order to use the \"*countVectorizer*\" feature. As we will need the labels later on, we will do it here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IwCboowF96T",
        "colab_type": "code",
        "outputId": "64f04c98-677a-463f-e8b9-91c43fb3e09e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Extragere labels (0/1) și Tweets pt countVectorizer\n",
        "train_labels = [train_labels[x].split('\\t')[1].replace(\"\\n\", '') for x in range(len(train_labels))]\n",
        "train_sentences = [train_samples[x].split('\\t')[1] for x in range(len(train_samples))]\n",
        "\n",
        "print(train_labels[0:10])\n",
        "print(train_sentences[0:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['1', '1', '1', '1', '0', '1', '0', '0', '0', '0']\n",
            "[\";%fE mr#& crmx temjc@m %'wb: }hHAm@@m ykm=aa Eje@ Ejh= EcrZk s}lZ$ rhfh }h@kofe@mk RgWE< >mfor@m @#@ m=hkaa TFr>o* h}Ah EHfm}e@mHk e#hj@ j&}k gAmaH mgmkafe cmT: k>.h XH(q! }FW @*oDgB #Sx.W hZ jh= chrZ }k#h svcNt ejmc@m gYAmZ efke@m h}Ah g@@m >m& }%k tr(: ;wxq Ere E*}ga hgZ h$mhr@m tkafe@m t@A %#sE =hkaa@m m*gH E@he=@m wk}hX Ejhr=@m Ejhr=@m h@mg:@\\n\", 'sAFW K#xk}t fH@ae m&Xd >h& @# l@Rd}a @Hc liT ehAr@m Xgmz !}a }eAr@m Be g@@m efH RB(D Ehk&\\n', 'zgHy% @kA qCrw h@@m he|%WA Eh}W@m mkZrmAah@ @(jh Nyz#b %ek$ jAmk Ae mghHrh &hAh khjkaf:@m hZ |%qg fC;q m}Ae mk}k#h AS<#*A A%h* fm}@m rH=a hcH}k N*mpC T& fm}@m fm}@@ Aef\\n', \"!ck& g@eAh =F; me @Hc Zk&} mk@eAhH jmjAafm >Cg' egj}A B#RhD A'D}X }k#h rx@ @mkA:@m Tp;& .dK> xdm jhgZ rDizK $AH }me# k: |%qg aZtHB CeD'; h}jH ;tws k: ahf} :@RcD }k#a >K@ki >h& r.#: hg Eh*hH nKD. @emkA@ jFYS Eh*hH @emkA HmHf:k yp(}% Eh}h= HmHf:\\n\", \"zpW hjreaek egae h: (AvnY }e m@p: EjfmZ @x<Yn Ehxr& Effoe =.'m} rmx% hZ $nhA qx&A #ib;( h}Ah =X*j m@p: DS:Xt &hZ SHq:t me EhchcA rhW k: KgZtn EhchcA }$e>c :|lb% cf}ek }e:@m ;U>Tv ymg= hZ h@ $!=qW cp@e =Z# %Nwt mH@mW h=x@H Wno| Akr&e @Bgh >mhc&:@mH #qpnt tefh me R@U% k: eg@#\\n\", \"gHp= &ARa |anU@ xvXeA pHX EAp UCnA Erm$A r!Ye N*ner @#k t<nAf vy( %pz* |X: %nUN faqr AaTF obmX' wZ> egarm$A Ax*wo}m hemf:@m\\n\", \"cK$ @m=e }e r.#: hZ m@emA :|lb% zCrT h@mkA Ah&ra E>TCa WfwD ea h@@m fH @eA Z@:@m acmZ @meA:@m aejc Ctv ha@m hg Ehmg}@@ @pk @eA E*mefe( }hH }e @he= rka#j@m ykZ h}Ah @h@j *'!X m*gk Hgmke@m am>mZ#\\n\", \"}R%Af Xgjq mAhe= amx@f@m A$=: rjpe HA&@m B#RhD }:H Errce@m tH% EjAmc@m f#Ah m@ h@mA@m NEx'( A$k@m }:@ EhrA@mk Ex@f@m }hH }ymka@m fboW jhAa }: H=h mg}: jcaA: *#la ha@m am@hjAa@m }e EAke=e *BbZU h}Ah h@ xzqABoD rjc m*% ejmc iaj= rgzW|X }me@rH h: hZ g}: @kc: m}: #@*@k\\n\", \"gAxH dzy} DENy' kdrCB ReW@j ej(afh ;EnNmb E=gH@m <p#o @eU Amt }#ee dzy} }xji' j(S iW&l h@@m HAf} h:r\\n\", 'X;d:N qnwB Acke@m m*g lvc& ggcp ht*A mat; }:@ HA&@m HA@e hZ Er#@m\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh2LEA-ZOobN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_51IJMpIT-T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Aplicăm countVectorizer pe Datele noastre\n",
        "'''\n",
        "fit_transform = fit + trasnform   AKA\n",
        "Învățăm vocabularul dicționarului și o transformăm în matrice (cuvinte-val)\n",
        "'''\n",
        "sentences_CV = CV.fit_transform(train_sentences).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbFJqMmPOwSR",
        "colab_type": "text"
      },
      "source": [
        "> # **Step 3.1:  Training / Testing**\n",
        "\n",
        "Now we can proceed with our project. We already have our dataset split into train, validation and test; so we don't need to interfere.\n",
        "\n",
        "We have to edit the test data so we can process it (we will separate the IDs from the tweets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiOLhPDvVLls",
        "colab_type": "code",
        "outputId": "cbb81a7a-2e7c-4f38-e539-94d56033cc9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Preprocesarea Datelor de Test\n",
        "\n",
        "# Extragere ID-uri din Setul de date pentru Test\n",
        "test_IDs = [test_samples[x].split('\\t')[0] for x in range(len(test_samples))]\n",
        "print(test_IDs[0:10])\n",
        "# Extragere Tweets din Setul de date pentru Test\n",
        "test_sentences = [test_samples[x].split('\\t')[1] for x in range(len(test_samples))]\n",
        "print(test_sentences[0:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['110499', '101319', '108883', '100925', '110852', '109538', '108874', '100082', '110470', '109725']\n",
            "[\"k>.h j:TW@ 'g cWUX }xDd fzsFU% zq|=p} <p#o #fEw ApziUd gjjAh &rk<' @he= Akefe@m jkjA HrWDpi hg @mof@m n(E&hj fZ} @:f} m}h@(\\n\", \":E(Kt mgheA: rjc: E*me@m hZ E*me k: Eg=@m mY*WpN }e:: rjc: m}: h@@m k: A$k@m t@A rxhf: rjc: E*me@m hZ E*me m}: xmh;! >y*p' E=mA ghZ tWNA me h}Ah >h& hZ me amek@Ae@m |r#$ >mz( }e: E@:fe hZ jw* }:@ gj# hy .jmkA@m hkaA} rjc} me cXoB :Dz%h jc @kmA}k mg}e frj #XSTd m}e@Aa m}A%\\n\", \"sXycp '#!@k h@A jeAZ fhkf@m A}Axvn m}mhA:k Ehrj}#f%@mk Ergmc@m t@A g@# yh#ra@m AHp:k @Amre@m HkpDa@ hZ gykm=a Rre( ea @sC h*@m }h#me fyT $ywZk' }e r.#: EAm}p@m Aykh g}% mj= wsgTF phrA }m# hAm}p@m Akr&e@m >m= :iks rSo*Yk me@ h@A jeAe\\n\", 'KYe mhrkfH Erk. g#>Y XWo !Hcn }k#a <=AzRy mg}: emh:@m H=hjF kr#z @Sgyh TzAS;: ekh\\n', \"CZH Eheh@Aa@m Effoe@m @(mj E$mhr frmeh g}% Erjc@m aHa#@ Ehfrje >mhrH: mk}m# >@UR UKj!Y lva!'h m*% gkH@mx }m#k r$m m$hre }k#h }: me% yAa }:@ jpre@m gH emc $Zip< meH\\n\", \"gZ& |b&Z }$%. tdi&z ;tws Hxowc h=hH ces> &e ZE& }hjAmc A'D}X h@@m Uc>A# gHp= obmX' FS>g T(l}$f FdUZ }h}xmke@m\\n\", \"xjZF yt'T} sYF nKD. !.CTv B#RhD j|nf :'oS& Xwe *q'oF Y%p*a amH@x @# a*Z} me Kjm% m*%k @.e (w>Kg hmg= #krHe t*Zg EjAmk zY.!A eAaK Y%aR( }q>AZK z(Ajt Ee@# HrlD g*g\\n\", 'EpYTZ% Eh*W:@m }e E}hA #aH=: cemW@m |fg #j}A me kem}ma}mkW hZ WmrZ@mk tkW@@m lY|.v Y@D|E Hr$ $%*h> }e r.#: Hr$:\\n', \"Xhb| =x!* gh@A U|p. c@x:k }R%Af }h@k*fe@m *=zE> jA: @(j (@;Zge :'BZ D*;dFv me m*% :'oS& FS>g h}Ah EHA@@m t<nAf &m&rH #feh p;Xh }m# Cpb> amre@m kjC ':* Xlfo tjA% fK>W hZ WZTEz @mc >m@A fC;q }:|!h jeAe\\n\", \"SURz E%<Hh jm@H@m hZ C*o=' mkHr=h eA poW &e $yCH| @zEK' ahH@m hZ }hrea@m m*g @*gn *sAg@ m}rxm&ak k;iRFr (fZak ghZ @eAa Nnzx *TiWf eym@ w$Ep h@@m Ecrk@m t@A &BlY ;@C(h 'SpbE dmX:t mhfmhf }hreafe Ecrk@m t@A }hrjmc |wc'Ux :<eim Cjc%g &e kg r#H kH:\\n\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0pe8la2YQO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transformăm setul de date pt Test și întoarcem matricea. (!!! NU facem fit)\n",
        "test_CV = CV.transform(test_sentences).toarray()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQst7yTSZwP7",
        "colab_type": "text"
      },
      "source": [
        "> # **Step 3.2:  Choosing the Best Algorithm**\n",
        "\n",
        "All is left for us to do is to determine which algorithm has the best accuracy for our dataset.\n",
        "\n",
        "We will start with:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp3Y3riVe5ie",
        "colab_type": "text"
      },
      "source": [
        "> # **Step 3.2.1:  Multinomial NaiveBayes**\n",
        "\n",
        "This particular classifier is suitable for classification with discrete features (such as in our case, word counts for text classification). It takes in integer word counts as its input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg8oUuJyhVri",
        "colab_type": "code",
        "outputId": "ab944bed-4bcf-48a8-d55f-ac007e1556d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Antrenare pe datele de Train cu Multinomial NB\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "NB = MultinomialNB()\n",
        "NB.fit(sentences_CV, train_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e202zbfIlBPC",
        "colab_type": "text"
      },
      "source": [
        "Now that our algorithm has been trained using the training data set we can now make some predictions on the test data. And we will save our predictions in a .csv file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdUNWcmQlE1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Facem prezicerile pentru setul de date pt Test\n",
        "prediction = NB.predict(test_CV)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PovmW6W8mn0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Salvare Predicții în fișier .csv separat\n",
        "import pandas as pd\n",
        "rezultat = pd.DataFrame(data={\"id\":test_IDs, \"label\": prediction} )\n",
        "rezultat.to_csv(\"Clasificator.csv\", index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH5hfkvDnESY",
        "colab_type": "text"
      },
      "source": [
        "Now that we have made predictions on our test set, our next goal is to evaluate how well our model is doing. For this, we will train our model on our validation set and we will compare the prediction with the actual labels. So we repeat the steps above, but for the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZP6GEg2nTJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extragere labels (0/1) și Tweets pt countVectorizer\n",
        "validation_labels = [validation_labels[x].split('\\t')[1].replace(\"\\n\", '') for x in range(len(validation_labels))]\n",
        "validation_sentences = [validation_samples[x].split('\\t')[1] for x in range(len(validation_samples))]\n",
        "\n",
        "\n",
        "# Transformăm setul de date pt Test și întoarcem matricea\n",
        "validation_CV = CV.transform(validation_sentences).toarray()\n",
        "\n",
        "\n",
        "# Facem prezicerile pentru setul de date pt Test\n",
        "prediction_validation = NB.predict(validation_CV)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfNA6cm-p5vp",
        "colab_type": "code",
        "outputId": "e9ebbf89-d855-40e1-eed2-a14638bafda0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Pentru a verifica acuratețea și f1-score\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "print('Accuracy score: ', accuracy_score(validation_labels, prediction_validation))\n",
        "print('F1 score: ', f1_score(validation_labels, prediction_validation, average='macro'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy score:  0.6701807228915663\n",
            "F1 score:  0.6648165222201733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8Sb4AZeu6R4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Matricea de Confuzie și Raport\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "cm = confusion_matrix(validation_labels, prediction_validation)\n",
        "r = classification_report(validation_labels, prediction_validation)\n",
        "\n",
        "# Afișare Matrice Confuzie\n",
        "print('Confusion Matrix :\\n')\n",
        "print(cm)\n",
        "\n",
        "# Afișare Raport\n",
        "print('\\n\\nReport :')\n",
        "print(r)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbnwDgnB1NhZ",
        "colab_type": "text"
      },
      "source": [
        "> # **Step 3.2.2:  Random Forest Classifier**\n",
        "\n",
        "An RFC algorithm creates many decision trees and combines them for a more stable and accurate prediction. In general, the more trees in the forest, the stronger the prediction and so we will get better accuracy. In the RFC, each decision tree expects an answer and the final answer is decided by vote. For classification problems, the answer received with the majority vote in the decision trees is the final answer. In an RFC, even if there are more trees, the model will not allow a case of overfitting.\n",
        "\n",
        "The algorithm will be similar with the one above, with a few changes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omy2MAKF2VxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Antrenare pe datele de Train cu RFC \n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "rfc = RandomForestClassifier(n_estimators=20000) \n",
        "rfc.fit(sentences_CV, train_labels) \n",
        "\n",
        "# Facem prezicerile pentru setul de date pt Test \n",
        "prediction = rfc.predict(test_CV)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}